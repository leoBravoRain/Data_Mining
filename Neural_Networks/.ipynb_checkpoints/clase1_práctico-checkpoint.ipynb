{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- Mejorar visualización en proyector -->\n",
       "<style>\n",
       ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
       "div.prompt {min-width: 0ex; padding: 0px;}\n",
       ".container {width:95% !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<!-- Mejorar visualización en proyector -->\n",
    "<style>\n",
    ".rendered_html {font-size: 1.2em; line-height: 150%;}\n",
    "div.prompt {min-width: 0ex; padding: 0px;}\n",
    ".container {width:95% !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red neuronal MLP con numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, input_dim=2, hidden_dim=10, rstate=None):\n",
    "        np.random.seed(rstate)        \n",
    "        assert hidden_dim >0, \"Neuronas ocultas debe ser mayor que cero\"\n",
    "        self.hidden_dim = hidden_dim\n",
    "        output_dim = 1 # Clasificación binaria\n",
    "        self.hidden_params = {'w':np.random.randn(input_dim, hidden_dim),\n",
    "                              'b':np.random.randn(hidden_dim)}\n",
    "        self.output_params = {'w': np.random.randn(hidden_dim, output_dim),\n",
    "                              'b': np.random.randn(output_dim)} \n",
    "\n",
    "    def forward(self, x, only_output=True):\n",
    "        z = logistic(np.dot(x, self.hidden_params['w']) + self.hidden_params['b'])  \n",
    "        y = logistic(np.dot(z, self.output_params['w']) + self.output_params['b'])\n",
    "        if only_output:\n",
    "            return y\n",
    "        else:\n",
    "            return z, y\n",
    "        \n",
    "    def score(self, x, y, eps=1e-10):        \n",
    "        yhat = self.forward(x)[:, 0] \n",
    "        logL = y*np.log(yhat+eps) + (1.0-y)*np.log(1.0-yhat+eps)\n",
    "        return -logL\n",
    "    \n",
    "    def backward(self, x, y, eta=1e-2):\n",
    "        zhat, yhat = self.forward(x, only_output=False)\n",
    "        # ¿A que corresponde la ecuación siguiente?\n",
    "        dL = -(y - yhat)  \n",
    "        self.output_params['w'] -= eta*np.dot(zhat.T, dL)\n",
    "        self.output_params['b'] -= eta*np.sum(dL) \n",
    "        # ¿A que corresponde la ecuación siguiente?\n",
    "        grad_z = dL*np.repeat(self.output_params['w'].T, len(dL), axis=0)*zhat*(1-zhat)\n",
    "        self.hidden_params['w'] -= eta*np.dot(x.T, grad_z)\n",
    "        self.output_params['b'] -= eta*np.sum(grad_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = MLP(hidden_dim=10)\n",
    "n_epochs, eta = 1000, 1e-2\n",
    "cost_history = np.zeros(shape=(n_epochs, 2))\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), tight_layout=True)\n",
    "\n",
    "def update_plot(k):\n",
    "    global nnet, cost_history    \n",
    "    cost_history[k, 0] = np.mean(nnet.score(data[train_idx], labels[train_idx]))\n",
    "    cost_history[k, 1] = np.mean(nnet.score(data[test_idx], labels[test_idx]))\n",
    "    [ax_.cla() for ax_ in ax]\n",
    "    Z = nnet.forward(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax[0].contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=1., vmin=0, vmax=1)\n",
    "    for i, (marker, label) in enumerate(zip(['o', 'x'], ['Train', 'Test'])):\n",
    "        ax[0].scatter(data[labels==i, 0], data[labels==i, 1], s=10, color='k', marker=marker, alpha=0.5)\n",
    "        ax[1].plot(np.arange(0, k+1, step=1), cost_history[:k+1, i], '-', label=label+\" cost\")\n",
    "    plt.legend(); ax[1].grid()\n",
    "    idx = np.random.permutation(len(train_idx))\n",
    "    for i in range(len(idx)//10):\n",
    "        idx_mb = train_idx[idx[i*10:(i+1)*10]]\n",
    "        nnet.backward(data[idx_mb, :], labels[idx_mb, np.newaxis], eta=eta)\n",
    "\n",
    "anim = animation.FuncAnimation(fig, update_plot, frames=n_epochs, \n",
    "                               interval=10, repeat=False, blit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios**\n",
    "- Experimente variando el número de capas, número de neuronas y tasa de aprendizaje. Comente sobre como se reflejan estas modificaciones en el desempeño de la red y en la  complejidad del hiperplano\n",
    "- Discuta sobre la relación entre complejidad del hiperplano, capacidad de generalización y sobreajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red neuronal MLP con [PyTorch](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 samples for train and 400 for testing\n"
     ]
    }
   ],
   "source": [
    "data, labels = sklearn.datasets.make_circles(n_samples=1000, noise=0.2, factor=0.25)\n",
    "\n",
    "# data, labels = sklearn.datasets.make_moons(n_samples=1000, noise=0.2)\n",
    "# data, labels = sklearn.datasets.make_blobs(n_samples=[250]*4, n_features=2, cluster_std=0.5,\n",
    "#                                           centers=np.array([[-1, 1], [1, 1], [-1, -1], [1, -1]]))\n",
    "labels[labels==2] = 1; labels[labels==3] = 0;\n",
    "\n",
    "train_idx, test_idx = next(sklearn.model_selection.ShuffleSplit(train_size=0.6).split(data, labels))\n",
    "print(\"%d samples for train and %d for testing\" %(len(train_idx), len(data)-len(train_idx)))\n",
    "# # Plot data\n",
    "# fig, ax = plt.subplots(figsize=(8, 5))\n",
    "# ax.scatter(data[labels==0, 0], data[labels==0, 1], marker='o', label=\"class 1\", alpha=0.5)\n",
    "# ax.scatter(data[labels==1, 0], data[labels==1, 1], marker='o', label=\"class 2\", alpha=0.5)\n",
    "# ax.set_xlabel('X1'); ax.set_ylabel('X2')\n",
    "# ax.grid(); plt.legend();\n",
    "\n",
    "# x_min, x_max = data[:, 0].min() - 0.5, data[:, 0].max() + 0.5\n",
    "# y_min, y_max = data[:, 1].min() - 0.5, data[:, 1].max() + 0.5\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/Desktop/master_UACH/env/lib/python3.6/site-packages/ipykernel_launcher.py:49: DeprecationWarning: Numeric-style type codes are deprecated and will result in an error in the future.\n",
      "/home/leo/Desktop/master_UACH/env/lib/python3.6/site-packages/ipykernel_launcher.py:50: DeprecationWarning: Numeric-style type codes are deprecated and will result in an error in the future.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Implementar red neuronal\n",
    "class myMLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=2, hidden_dim=2, output_dim=1):        \n",
    "        super(myMLP, self).__init__()  \n",
    "        # Completar aquí\n",
    "        \n",
    "        # hidden layer\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        \n",
    "#         # output layer\n",
    "#         self.output = nn.Linear(output_dim, output_dim)\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Completar aquí\n",
    "        \n",
    "        print('forward')\n",
    "        \n",
    "        # recibe inputs and apply linear transformation\n",
    "        x = self.hidden(x) # input to hidden layer\n",
    "        \n",
    "        # apply activation function\n",
    "        x = torch.sigmoid(x) # output of hidden layer\n",
    "        \n",
    "        # apply output layer\n",
    "        x = self.output(x) # input to output layer\n",
    "        \n",
    "        # apply activation function\n",
    "        x = torch.sigmoid(x) # output of output layer\n",
    "        \n",
    "        # return output\n",
    "        return x\n",
    "\n",
    "# create NN\n",
    "net = myMLP()\n",
    "\n",
    "# Crear conjuntos de entrenamiento y prueba\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset \n",
    "\n",
    "\n",
    "# torch_set = TensorDataset(torch.from_numpy(data.astype('Float32')), \n",
    "torch_train_loader = TensorDataset(torch.from_numpy(data.astype('Float32')), \n",
    "                          torch.from_numpy(labels.astype('Float32')))\n",
    "# torch_train_loader = torch_set\n",
    "# torch_train_loader = DataLoader(Subset(torch_set, train_idx), shuffle=True, batch_size = 32)\n",
    "# torch_valid_loader = DataLoader(Subset(torch_set, test_idx), shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# cost function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer for update params\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.95806767 -1.31824587]\n",
      " [-0.12595041  0.8051662 ]\n",
      " [ 0.15636833  0.22791958]\n",
      " ...\n",
      " [ 0.20951776  0.51625755]\n",
      " [ 0.44915054 -0.83304126]\n",
      " [ 0.34050221 -0.88307042]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-562544c68610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 942\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2054\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/master_UACH/env/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# train\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data_ in enumerate(torch_train_loader, 0):\n",
    "        \n",
    "        print(data)\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data_\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "#         forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio:** \n",
    "- Complete la implementación de `myMLP`, construya una red neuronal completamente conectada con una capa oculta\n",
    "- Entrene su red por 1000 épocas usando Adam, proponga un criterio para detener el entrenamiento de la red que ayude a evitar el sobreajuste. Justifique\n",
    "- Haga pruebas con un tamaño de minibatch de entrenamiento de 1, 32 y 256 ¿Qué diferencias nota? Justifique\n",
    "- Experimente con los datasets *two moons* y *blobs* usando distinto número de neuronas en la capa oculta y distintas funciones de activación ¿Qué diferencias puede reportar?\n",
    "- Opcional: Modifique el entrenamiento para correr en GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3.5), tight_layout=True)\n",
    "\n",
    "n_epochs = 1000\n",
    "net = myMLP(input_dim=2, hidden_dim=100, output_dim=1)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "running_loss = np.zeros(shape=(n_epochs, 2))\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "def train_one_epoch(net):\n",
    "    train_loss, valid_loss = 0.0, 0.0\n",
    "    for sample_data, sample_label in torch_train_loader:\n",
    "        output = net(sample_data)\n",
    "        optimizer.zero_grad()        \n",
    "        loss = criterion(output[:, 0], sample_label)  \n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    for sample_data, sample_label in torch_valid_loader:\n",
    "        output = net(sample_data)\n",
    "        loss = criterion(output[:, 0], sample_label)  \n",
    "        valid_loss += loss.item()\n",
    "    return train_loss/torch_train_loader.__len__(), valid_loss/torch_valid_loader.__len__()\n",
    "    \n",
    "def update_plot(k):\n",
    "    global net, running_loss\n",
    "    [ax_.cla() for ax_ in ax]\n",
    "    running_loss[k, 0], running_loss[k, 1] = train_one_epoch(net)\n",
    "    Z = net.forward(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "    Z = sigmoid(Z).detach().numpy().reshape(xx.shape)\n",
    "    ax[0].contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=1., vmin=0, vmax=1)\n",
    "    for i, (marker, label) in enumerate(zip(['o', 'x'], ['Train', 'Test'])):\n",
    "        ax[0].scatter(data[labels==i, 0], data[labels==i, 1], color='k', s=10, marker=marker, alpha=0.5)\n",
    "        ax[1].plot(np.arange(0, k+1, step=1), running_loss[:k+1, i], '-', label=label+\" cost\")\n",
    "    plt.legend(); ax[1].grid()\n",
    "    \n",
    "anim = animation.FuncAnimation(fig, update_plot, frames=n_epochs, \n",
    "                               interval=10, repeat=False, blit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opcional: Implementación usando tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "from os.path import join\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf_input = tf.placeholder(tf.float32, [None, 2], name='input')\n",
    "tf_label = tf.placeholder(tf.float32, [None, 1], name='target')\n",
    "\n",
    "Nh = 10\n",
    "nepochs = 500  \n",
    "\n",
    "with tf.variable_scope('Hidden_layer'):\n",
    "    bh = tf.Variable(tf.zeros([Nh]), name=\"bias\", dtype=tf.float32)\n",
    "    wh = tf.Variable(tf.random_uniform([2, Nh], -1.0, 1.0), name=\"weight\", dtype=tf.float32)\n",
    "    z = tf.nn.tanh(tf.matmul(tf_input, wh) + bh)\n",
    "\n",
    "with tf.variable_scope('Output_layer'):\n",
    "    bo = tf.Variable(tf.zeros([1]), name=\"bias\", dtype=tf.float32)\n",
    "    wo = tf.Variable(tf.random_uniform([Nh, 1], -1.0, 1.0), name=\"weight\", dtype=tf.float32)\n",
    "    y = tf.add(tf.matmul(z, wo), bo)\n",
    "\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_label, logits=y)\n",
    "    loss_op = tf.reduce_mean(cross_entropy)  \n",
    "    optimizer = tf.train.AdamOptimizer(1e-2)\n",
    "    train_op = optimizer.minimize(loss_op) \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss_op)\n",
    "    tf.summary.histogram('output_weight', wo)\n",
    "    tf.summary.histogram('output_bias', bo)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios:** \n",
    "- Visualice el grafo, las curvas de aprendizaje y los histogramas de parámetros usando la herramienta tensorboard\n",
    "- Modifique el código que genera el grafo para agregar una segunda capa oculta\n",
    "- Estudie la función de mayor abstracción tf.layers.dense y usela para modificar el código que genera el grafo\n",
    "- ¿Cómo modificaría el código de entrenamiento para usar mini-batches?\n",
    "\n",
    "**Instrucciones tensorboard**\n",
    "1. Ejecutar: tensorboard --logdir /tmp/tensorboard/ \n",
    "2. Apuntar el navegador a localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = join(\"/tmp/tensorboard/\", str(time.time()))\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(join(log_dir, 'train'), sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(join(log_dir, 'test'), sess.graph)\n",
    "    sess.run(init)\n",
    "    train_loss = np.zeros(shape=(nepochs))\n",
    "    test_loss = np.zeros(shape=(nepochs))\n",
    "    for i, epoch in enumerate(range(nepochs)):\n",
    "        # run the training operation\n",
    "        _, train_loss[i], summary = sess.run([train_op, loss_op, merged], feed_dict={tf_input: data_train, \n",
    "                                                         tf_label: np.reshape(labels_train, [-1, 1])})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        pred_test, test_loss[i], summary = sess.run([y, loss_op, merged], feed_dict={tf_input: data_test, \n",
    "                                                                 tf_label: np.reshape(labels_test, [-1, 1])})\n",
    "        test_writer.add_summary(summary, i)\n",
    "\n",
    "    Z = sess.run(y, feed_dict={tf_input: (np.c_[xx.ravel(), yy.ravel()]).astype('float32')})\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=0.75)\n",
    "ax.scatter(data[labels==0, 0], data[labels==0, 1], color='k', marker='o', alpha=0.5)\n",
    "ax.scatter(data[labels==1, 0], data[labels==1, 1], color='k', marker='x', alpha=0.5)\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(train_loss, label='Train loss', linewidth=2)\n",
    "ax.plot(test_loss, label='Test loss', linewidth=2)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
