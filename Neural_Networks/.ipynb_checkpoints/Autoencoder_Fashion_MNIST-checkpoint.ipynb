{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Escriba un autoencoder y entrénelo sobre el dataset [Fashion-MNIST](https://pytorch.org/docs/stable/torchvision/datasets.html#fashion-mnist)\n",
    "    - Use capas convolucionales para el encoder y el decoder\n",
    "    - Usted debe definir la cantidad de capas, la cantidad de filtros por capa y el tamaño de los filtros\n",
    "    - Use 2 dimensiones en el cuello de botella (espacio latente)\n",
    "    - Use activación lineal en el cuello de botella y en la salida\n",
    "    - Use función de costo `BCEWithLogitsLoss`\n",
    "1. Visualice el espacio latente del conjunto de test con un scatter plot (coloree cada clase de forma distinta). Describa el resultado y comente sobre las clases más separadas y más \"confundidas\" en el espacio latente.\n",
    "1. Encuentre y visualize los diez ejemplos con mayor error de reconstrucción del conjunto de test\n",
    "1. Mida la calidad de las reconstrucciones en el conjunto de test por clase. ¿Cúales clases son más difíciles de reconstruir?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sklearn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import sklearn.model_selection\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train data\n",
    "train_data = torchvision.datasets.FashionMNIST('mnist_fashion_dataset', \n",
    "                                               train = True, \n",
    "                                               transform = torchvision.transforms.ToTensor(), \n",
    "                                               download = True)\n",
    "\n",
    "# Get test data\n",
    "test_data = torchvision.datasets.FashionMNIST('mnist_fashion_dataset', \n",
    "                                       train=False, \n",
    "                                       download=True,\n",
    "                                       transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  Shirt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f25bc8c4fd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAUEklEQVR4nO3da2yVVboH8P/DXQsFKpeWmwy3KJ54kADeCMrlCGoUJxqjxiPHEJkPQzIqxKMedfxgjDGHMRM9Y4KKMAYkoyOXKHimEhWNOlCIh0uRyyFVWgsFuaeF0vKcD32dU7Xv89T97r3fbdf/lzRt97+re+23ffru7vWutURVQUQdX6e0O0BE+cFiJwoEi50oECx2okCw2IkC0SWfdyYiOXvpv1On9P5uiUhO2zc1NZm59diLi4vNtt26dTPz8+fPm/nJkyfNvLGxMTbr3Lmz2dbjjSRZxzXtUSjvuCahqm0+8ETFLiKzAPwRQGcAr6rqc0m+XxJFRUVp3bX7h8Yr5i5d7B/DkSNHzNx67FOnTjXbDh061MwbGhrMvLy83Myrqqpis549e5ptPc3NzWZu/THJZbG1x6lTp/J+nxmfDkWkM4D/AnAjgLEA7haRsdnqGBFlV5LnvpMA7FPV/araCGAlgNnZ6RYRZVuSYh8M4ECrz6uj235AROaJSIWIVCS4LyJKKOcv0KnqYgCLgdy+QEdEtiRn9hoArV/dGRLdRkQFKEmxbwYwWkR+JSLdANwFYG12ukVE2Zbx03hVbRKR+QD+Gy1Db0tUdWfWepZnScZdvSGg06dPZ/y9AX947JFHHonNBg0aZLbt27evmffq1cvMr732WjN/7LHHYrPa2lqzradr165m3qNHj9gsyRj9L1Wi/9lVdR2AdVnqCxHlEC+XJQoEi50oECx2okCw2IkCwWInCgSLnSgQeZ3PniZvXNWbW23N2/a+tzfN1BvLfv7558184cKFsdmcOXPMtsuWLTNzr/3mzZsz/v7V1dVm2z/96U9mXlFhT7ewrn/wpkR3xHF4ntmJAsFiJwoEi50oECx2okCw2IkCwWInCkQwQ2+eEydOmLk1nXLatGlm2xdffNHMV65caeYbN24089LS0tisT58+Ztv+/fub+blz58y8vr7ezHfs2BGbVVZWmm1nzZpl5ldffbWZW8fdW93VGw71VqdNc2nzOIXXIyLKCRY7USBY7ESBYLETBYLFThQIFjtRIFjsRIHoMOPs3pREb9tjj7Uk88yZM822a9asMfN9+/aZ+YABA8z88OHDsZk1Bg/4480DBw40c2+8uaysLDb74osvzLbTp08385oae0+S5cuXx2b33nuv2dY7Lt71C971CWngmZ0oECx2okCw2IkCwWInCgSLnSgQLHaiQLDYiQLRYcbZve17vW2TH3roITMfP358bGZtDQwA27ZtM/OGhgYz7969u5lbc/HXrl1rtt25095le//+/WZeV1dn5gcPHozNvOWYvesThg0bZubWdtTz5883265fv97MvWsjevbsaeZpSFTsIlIF4BSAZgBNqjohG50iouzLxpl9qqoeycL3IaIc4v/sRIFIWuwK4G8iskVE5rX1BSIyT0QqRMTeq4eIcirp0/jJqlojIgMAlIvIV6r6g9URVXUxgMUAICL2bBUiyplEZ3ZVrYne1wFYBWBSNjpFRNmXcbGLSJGI9Pr+YwA3AIhfN5iIUpXkafxAAKuisdIuAFao6vtZ6VUGjh07ZuYlJSVmbo2jA0BjY2NstmLFCrPtfffdZ+a9e/c2c29utDW3+pJLLjHbWuu6A/Y4OeCvE1BcXBybeT+TyZMnm/lXX31l5nv27InNbr31VrOt9/tw//33m7l3XUcaMi52Vd0P4J+z2BciyiEOvREFgsVOFAgWO1EgWOxEgWCxEwWiw0xx9XjTRL1lia2hvRdeeMFsu2jRIjOvrq4289mzZ5v5mTNnYjNv+q03vXbUqFFmfuGFF5q5NfTmDeu9/PLLZv7SSy+Z+YcffhibVVTYV297x+WJJ54w82eeecbM08AzO1EgWOxEgWCxEwWCxU4UCBY7USBY7ESBYLETBSKYcfZXX33VzOvr683c2sJ3165dZtspU6aY+ejRo83cG4c/ciR+vU9vqefa2lozP3DggJlXVVWZubUU9ZAhQ8y299xzj5m/8847Zm5N77W2uQaASZPsdVhWrVpl5r169TJzb0voXOCZnSgQLHaiQLDYiQLBYicKBIudKBAsdqJAsNiJAhHMOPvbb79t5kOHDjXziRMnxmb9+vUz21rj4ADwxhtvmLk35mttTez1bcyYMWZeWlqaqP2VV14Zm3nz2SsrK828qKjIzL2+WV555RUzX716tZlbawykhWd2okCw2IkCwWInCgSLnSgQLHaiQLDYiQLBYicKRDDj7K+//rqZe+Pw5eXlsdk333xjtrW2VAaAp556ysw91jh9c3Oz2dbb6tq7RsB77Na2ySNGjDDbzpgxw8y99fqtvlv9AoBnn33WzFeuXGnm0VbmsVTVzHPBPbOLyBIRqRORHa1uKxGRchHZG72Pv6qDiApCe57GLwUw60e3PQpgg6qOBrAh+pyICphb7Kq6EcDRH908G8Cy6ONlAG7Lcr+IKMsy/Z99oKp+v3jZQQAD475QROYBmJfh/RBRliR+gU5VVURiX21Q1cUAFgOA9XVElFuZDr0dEpEyAIje20uYElHqMi32tQDmRB/PAbAmO90holxxn8aLyJsArgfQT0SqAfwewHMA/iIicwF8DeDOXHayPTp1sv9unT9/3sxPnz5t5g888EBstm3bNrOtt/75xo0bzfy9994zc2vutLc+ec+ePc28d+/eZj5o0CAzt9YJmDt3rtl2+vTpZr5gwQIz//jjj2Mzr9/Wevft4R3XNNaNd4tdVe+OieyfBBEVFF4uSxQIFjtRIFjsRIFgsRMFgsVOFIgOM8XVm1Lo8aYcbtq0KTZbsWKF2XbUqFFmPmDAADP3hqAaGhpis8GDB5tte/ToYeaNjY1m7mlqaorNvCmu3vCU9zO3tlUeP3682fbdd981c483FJyGwusREeUEi50oECx2okCw2IkCwWInCgSLnSgQLHaiQHCcPVJdXW3m1lj4ww8/bLbt3r27mXvLWH/77bdmvn379tjs4osvNtt6j/vs2bNm7rH67o1Fe8t/e0twz5w5M6MM8H9mGzZsMPOkv4+5wDM7USBY7ESBYLETBYLFThQIFjtRIFjsRIFgsRMFosOMsyfdAve6664z8yVLlsRmY8eONduOHDnSzEtKSsz8xhtvNPM1a+KX7Z88ebLZtra21sy9paRLS0vNvKysLDbz5pTfcMMNZu79zK1rCD744AOz7dq1a83ck8aWzB6e2YkCwWInCgSLnSgQLHaiQLDYiQLBYicKBIudKBAdZpy9uLjYzI8dO2bmH330kZlb669fccUVZtuuXbua+fLly8187969Zv7pp5/GZt26dTPb7t6928ytLZcBv2/W1sg7duww215wwQVm7l0DYPX9lltuMdu+9dZbZu75Rc5nF5ElIlInIjta3fa0iNSIyJfR20257SYRJdWep/FLAcxq4/YXVHVc9LYuu90iomxzi11VNwI4moe+EFEOJXmBbr6IbIue5veN+yIRmSciFSJSkeC+iCihTIv9ZQAjAYwDUAtgUdwXqupiVZ2gqhMyvC8iyoKMil1VD6lqs6qeB/AKgEnZ7RYRZVtGxS4irect/hqAPYZCRKlzx9lF5E0A1wPoJyLVAH4P4HoRGQdAAVQB+E0O+9guSecPf/LJJ2Y+fPjw2Mxb133IkCFmftddd5n5rFltDYb8v2HDhsVm3nz28+fPm7nXvqamxsynTJkSm3lrs0+bNs3M6+rqzLyiIv5lov79+5ttjx8/buaeQpzP7ha7qt7dxs2v5aAvRJRDvFyWKBAsdqJAsNiJAsFiJwoEi50oEB1mimtS3nTKcePGxWYjRoww21ZWVpr5ihUrzHzfvn1m/tlnn8Vmhw8fNtvu3LnTzPv2jb0SGgDw+eefm3lTU1Ns5h0Xb1jPGx6ztqv2psf26dPHzD3edtRpKLweEVFOsNiJAsFiJwoEi50oECx2okCw2IkCwWInCgTH2SPeePLRo/HL8G3fvt1sO3XqVDN/8MEHzdxbqtq6BsBr6411X3755WZ+5swZM7emuG7ZssVs601xXbp0qZlby1x7S2TX19ebuacQp7jyzE4UCBY7USBY7ESBYLETBYLFThQIFjtRIFjsRIHgOHvEG1e1th7u1auX2dbbDnr9+vVmvmbNGjO3xpNHjhxptj1x4oSZjxkzxszff/99M9+zZ09s5s3T379/v5lv3brVzK+55prYzFve+6KLLjJzD8fZiSg1LHaiQLDYiQLBYicKBIudKBAsdqJAsNiJAtFhxtm7dEn2ULyti8+ePRubWWPJgD9XfuHChWbujflWV1fHZqWlpWZb73FfeumlZj569Ggzv/3222OzTZs2mW2vuuoqM1+wYIGZl5eXx2be9QPfffedmXtEJFH7XHDP7CIyVEQ+FJFKEdkpIr+Lbi8RkXIR2Ru9t3+jiShV7Xka3wRggaqOBXAVgN+KyFgAjwLYoKqjAWyIPieiAuUWu6rWqurW6ONTAHYBGAxgNoBl0ZctA3BbrjpJRMn9rH90RWQ4gCsA/B3AQFWtjaKDAAbGtJkHYF7mXSSibGj3q/Ei0hPAXwE8qKonW2factV/m1f+q+piVZ2gqhMS9ZSIEmlXsYtIV7QU+nJVfSe6+ZCIlEV5GYC63HSRiLLBfRovLWMIrwHYpap/aBWtBTAHwHPRe3seZo5ZQ2PtYW17DAA333xzbOYNra1evdrMd+/ebebedMna2trYrHv37mZbb4jIW3LZm+JqbQntDb1NmGA/GfSmyF522WWxmbU0OGAf0/YoxCmu7fmf/VoA/wpgu4h8Gd32OFqK/C8iMhfA1wDuzE0XiSgb3GJX1U8BxP35n57d7hBRrvByWaJAsNiJAsFiJwoEi50oECx2okB0mCmuHm8KrDeuevr06djMG1OdOHGimd9xxx1mXlVVZebnzp2LzbwprMXFxWbujdP369fPzK0tn4cPH262ta5tAPytrmfMmBGbeY/bm7pbU1Nj5p06Fd55tPB6REQ5wWInCgSLnSgQLHaiQLDYiQLBYicKBIudKBAdZpy9c+fOOf3+1ha/3njxokWLzHzdunVmbo0XA8Dx48djM29LZm8dgJKSEjP3jntjY2Ns5s2l99YB2LBhg5lb1y+cOnXKbNujRw8z9xTifHae2YkCwWInCgSLnSgQLHaiQLDYiQLBYicKBIudKBCSz/FAEcnZnRUVFZm59zjr6+vN/Mknn4zNDh06ZLZdunSpmVtj0b901jh8c3NzTu973LhxsdnJkydjMwBoamoy8wMHDpi5N1/eu/4hCVVt8wIGntmJAsFiJwoEi50oECx2okCw2IkCwWInCgSLnSgQ7ji7iAwF8GcAAwEogMWq+kcReRrAAwAOR1/6uKqaE7NzOc7ep08fM7fWVm9PnsuxcO8agYaGhoy/t7d+uZd7vx9ebo2le2v5d+vWLePvDQBnzpwxc4vXN+9n5q3X782nTyJunL09i1c0AVigqltFpBeALSJSHmUvqOp/ZquTRJQ77dmfvRZAbfTxKRHZBWBwrjtGRNn1s/5nF5HhAK4A8Pfopvkisk1ElohI35g280SkQkQqEvWUiBJpd7GLSE8AfwXwoKqeBPAygJEAxqHlzN/mQmuqulhVJ6jqhCz0l4gy1K5iF5GuaCn05ar6DgCo6iFVbVbV8wBeATApd90koqTcYpeWJUBfA7BLVf/Q6vayVl/2awA7st89IsqW9gy9TQbwCYDtAL4fT3gcwN1oeQqvAKoA/CZ6Mc/6XqlNcfWGUrxhHGtpYe8YetMlvdzru7Ukczt+vonyJN/fe9xJWcct6ePyhtZ+kUNvqvopgLYa24udE1FB4RV0RIFgsRMFgsVOFAgWO1EgWOxEgWCxEwWiw2zZ7I2beuPoXntva+Mk9921a1cz98ZskywH7j3uXzLvuFmSLnNdiMeVZ3aiQLDYiQLBYicKBIudKBAsdqJAsNiJAsFiJwpEvrdsPgzg61Y39QNwJG8d+HkKtW+F2i+AfctUNvt2sar2byvIa7H/5M5FKgp1bbpC7Vuh9gtg3zKVr77xaTxRIFjsRIFIu9gXp3z/lkLtW6H2C2DfMpWXvqX6PzsR5U/aZ3YiyhMWO1EgUil2EZklIrtFZJ+IPJpGH+KISJWIbBeRL9Peny7aQ69ORHa0uq1ERMpFZG/0vs099lLq29MiUhMduy9F5KaU+jZURD4UkUoR2Skiv4tuT/XYGf3Ky3HL+//sItIZwB4A/wKgGsBmAHeramVeOxJDRKoATFDV1C/AEJEpAE4D+LOq/lN02/MAjqrqc9Efyr6q+u8F0renAZxOexvvaLeistbbjAO4DcC/IcVjZ/TrTuThuKVxZp8EYJ+q7lfVRgArAcxOoR8FT1U3Ajj6o5tnA1gWfbwMLb8seRfTt4KgqrWqujX6+BSA77cZT/XYGf3KizSKfTCAA60+r0Zh7feuAP4mIltEZF7anWnDwFbbbB0EMDDNzrTB3cY7n360zXjBHLtMtj9Pii/Q/dRkVR0P4EYAv42erhYkbfkfrJDGTtu1jXe+tLHN+D+keewy3f48qTSKvQbA0FafD4luKwiqWhO9rwOwCoW3FfWh73fQjd7Xpdyffyikbbzb2mYcBXDs0tz+PI1i3wxgtIj8SkS6AbgLwNoU+vETIlIUvXACESkCcAMKbyvqtQDmRB/PAbAmxb78QKFs4x23zThSPnapb3+uqnl/A3ATWl6R/18A/5FGH2L6NQLA/0RvO9PuG4A30fK07hxaXtuYC+AiABsA7AXwAYCSAurbG2jZ2nsbWgqrLKW+TUbLU/RtAL6M3m5K+9gZ/crLcePlskSB4At0RIFgsRMFgsVOFAgWO1EgWOxEgWCxEwWCxU4UiP8DPF+q8x9y4S8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get image\n",
    "image = train_data[random.randint(0, len(train_data))]\n",
    "\n",
    "print('Label: ', classes[image[1]])\n",
    "\n",
    "# plot image\n",
    "plt.figure()\n",
    "plt.imshow(image[0].numpy()[0, :, :], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spliting data ids\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(train_size = 0.6)\n",
    "train_idx, valid_idx = next(sss.split(np.zeros(len(train_data)), train_data.targets))\n",
    "\n",
    "# Get subset for training\n",
    "train_dataset = Subset(train_data, train_idx)\n",
    "\n",
    "# Get dataloader for trainnig\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "# Get subset for validation\n",
    "valid_dataset = Subset(train_data, valid_idx)\n",
    "\n",
    "# Get dataloader for validation\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=256)\n",
    "\n",
    "# Get test data loader\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use capas convolucionales para el encoder y el decoder\n",
    "- Usted debe definir la cantidad de capas, la cantidad de filtros por capa y el tamaño de los filtros\n",
    "- Use 2 dimensiones en el cuello de botella (espacio latente)\n",
    "- Use activación lineal en el cuello de botella y en la salida\n",
    "- Use función de costo BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(torch.nn.Module):\n",
    "    \n",
    "    # Define architecture\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(autoencoder, self).__init__()\n",
    "         \n",
    "        # Encoder\n",
    "        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        # Activation\n",
    "        self.act = torch.nn.ReLU()\n",
    "        \n",
    "        # Latent \n",
    "        self.latent = torch.nn.Linear(1960, 2)\n",
    "        \n",
    "        # Decoder\n",
    "        self.deconv1 = torch.nn.ConvTranspose2d(1, 1, kernel_size = 10, stride = 3)\n",
    "#         self.deconv2 = torch.nn.ConvTranspose2d()\n",
    "        \n",
    "    \n",
    "    def encode(self, x, batch_size):\n",
    "        \n",
    "#         # size of minibatch\n",
    "#         input_size = x.size(0) \n",
    "        \n",
    "#         print(input_size)\n",
    "        \n",
    "        # Conv 1\n",
    "        tmp = self.act(self.conv1(x))\n",
    "        \n",
    "        print('out conv 1: ', tmp.size())\n",
    "        \n",
    "        # Pool\n",
    "        tmp = self.pool1(tmp)\n",
    "        \n",
    "        print('out pool 1: ', tmp.size())\n",
    "        \n",
    "        # flatten (reshape)\n",
    "        tmp = tmp.view(batch_size, -1)\n",
    "        \n",
    "        print('out flatten: ', tmp.size())\n",
    "        \n",
    "        # Latent\n",
    "        tmp = self.act(self.latent(tmp))\n",
    "        \n",
    "        print('out latent: ', tmp.size())\n",
    "        \n",
    "        return tmp\n",
    "        \n",
    "    def decode(self, z, batch_size):\n",
    "        # Llenar\n",
    "        \n",
    "        print('input decode: ', z.size())\n",
    "        \n",
    "#         tmp = z.unsqueeze(0).unsqueeze(0)\n",
    "#         tmp[0] = batch_size\n",
    "        \n",
    "        print('output unsqueeze: ', tmp.size())\n",
    "        \n",
    "#         tmp = tmp.view(batch_size, 1, 1, 1)\n",
    "        \n",
    "        print('output reshape')\n",
    "        \n",
    "        tmp = self.act(self.deconv1(tmp))\n",
    "        \n",
    "        print('output deconv1: ', tmp.size())\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size()[0]\n",
    "        \n",
    "        print('input size: ', x.size())\n",
    "        \n",
    "        # Encode\n",
    "        tmp = self.encode(x, batch_size)\n",
    "        \n",
    "        # Decode\n",
    "        tmp = self.decode(tmp, batch_size)\n",
    "        \n",
    "#         print(tmp.size())\n",
    "        \n",
    "        return tmp\n",
    "#         return self.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "\n",
    "        ## encoder layers ##\n",
    "        # conv layer (depth from 1 --> 16), 3x3 kernels\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
    "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        ## decoder layers ##\n",
    "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## encode ##\n",
    "        # add hidden layers with relu activation function\n",
    "        # and maxpooling after\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # add second hidden layer\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # compressed representation\n",
    "\n",
    "        ## decode ##\n",
    "        # add transpose conv layers, with relu activation function\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        # output layer (with sigmoid for scaling from 0 to 1)\n",
    "        x = torch.sigmoid(self.t_conv2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test if the forward method is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "image = train_data[random.randint(0, len(train_data))][0]\n",
    "\n",
    "print(image.size())\n",
    "\n",
    "model = autoencoder()\n",
    "\n",
    "y = model.forward(image.unsqueeze(0))\n",
    "\n",
    "# print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set model and its configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = autoencoder()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "543.4273681640625\n",
      "1\n",
      "543.4273681640625\n",
      "2\n",
      "543.4273681640625\n",
      "3\n",
      "543.4273681640625\n",
      "4\n",
      "543.4273681640625\n",
      "5\n",
      "543.4273681640625\n",
      "6\n",
      "543.4273681640625\n",
      "7\n",
      "543.4273681640625\n",
      "8\n",
      "543.4273681640625\n",
      "9\n",
      "543.4273681640625\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "nepochs = 10\n",
    "\n",
    "if use_gpu:\n",
    "    \n",
    "    nnet = model.cuda()\n",
    "\n",
    "import time\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# tensorboard --logdir=/tmp/tensorboard\n",
    "current_time = str(time.time())\n",
    "train_writer = SummaryWriter(\"/tmp/tensorboard/digits/train/\", flush_secs=10)\n",
    "valid_writer = SummaryWriter(\"/tmp/tensorboard/digits/valid/\", flush_secs=10)\n",
    "\n",
    "for epoch in range(nepochs): \n",
    "    \n",
    "    print(epoch)\n",
    "    \n",
    "    # Train\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for mbdata, mblabel in train_loader:\n",
    "        \n",
    "        if use_gpu:\n",
    "            \n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "            \n",
    "        prediction = model(mbdata)\n",
    "        \n",
    "        optimizer.zero_grad()    \n",
    "               \n",
    "        loss = criterion(prediction, mbdata)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    print(epoch_loss/len(train_idx))\n",
    "    train_writer.add_scalar('Loss', epoch_loss/len(train_idx), epoch)\n",
    "    \n",
    "    # Validation    \n",
    "    epoch_loss = 0.0\n",
    "    for mbdata, mblabel in valid_loader:\n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        prediction = model(mbdata)\n",
    "        loss = criterion(prediction, mbdata)\n",
    "        epoch_loss += loss.item()\n",
    "    valid_writer.add_scalar('Loss', epoch_loss/len(valid_idx), epoch)\n",
    "\n",
    "if use_gpu:\n",
    "    nnet = nnet.cpu()\n",
    "    \n",
    "train_writer.close()\n",
    "valid_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandal\n",
      "torch.Size([1, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2580b3efd0>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "image = train_data[random.randint(0,1000)]\n",
    "\n",
    "print(classes[image[1]])\n",
    "\n",
    "y = model.forward(image[0].unsqueeze(0))\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.imshow(y.detach()[0,0, :, :], cmap = 'gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
