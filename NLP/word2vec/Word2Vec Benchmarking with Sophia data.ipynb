{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisis\n",
    "Entrenar un modelo Word2Vec para el español. Se puede utilizar agrupar datasets de \"Sophia\" en un solo dataset grande.\n",
    "\n",
    "Entrenar distintos modelos de clasificación de textos, utilizando Word2Vec y sin utilizar Word2Vec, para resolver el problema de clasificación siguiente: reconocer automáticamente el medio qlas noticias según el nombre del medio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression # Log classifier\n",
    "from sklearn.model_selection import train_test_split # Split in train/test\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim \n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit # for split data (train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Detectamos que JavaScript está desactivado en...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Detectamos que JavaScript está desactivado en...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Detectamos que JavaScript está desactivado en...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Para los diputados, el informe de auditoría q...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ayer se concretó el cierre del primer Centro ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              report  company\n",
       "0   Detectamos que JavaScript está desactivado en...        1\n",
       "1   Detectamos que JavaScript está desactivado en...        1\n",
       "2   Detectamos que JavaScript está desactivado en...        1\n",
       "3   Para los diputados, el informe de auditoría q...        1\n",
       "4   Ayer se concretó el cierre del primer Centro ...        1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analize data\n",
    "df = pd.read_csv('dataset-cuarta_vs_tercera.csv', sep = '|', header = None)\n",
    "\n",
    "# Get news and medium\n",
    "df = df.iloc[:,[1,3]]\n",
    "\n",
    "# display(df.iloc[1,:])\n",
    "\n",
    "# Get dummies variables\n",
    "dummies = pd.get_dummies(df[1], drop_first = True)\n",
    "df = pd.concat([df, dummies], axis=1)      \n",
    "df.drop([1], inplace=True, axis=1)\n",
    "\n",
    "# Add names\n",
    "df.columns = ['report', 'company']\n",
    "\n",
    "# company:\n",
    "# 0: La cuarta\n",
    "# 1: La tercera\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,   0.,   0.,   0.,   0., 100.,   0.,   0.,   0.,   0.]),\n",
       " array([0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3, 1.4, 1.5]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAP7ElEQVR4nO3df+xddX3H8edrVGDqlGK7iqC2xG4GzRTWMPwRp2Ai4mJZZhxGt+q6dP6cziUTRzKdyTJwy1CzRdOAWjeDYOcG88c2LBizOOqKovwSKSAKK7Qq4JiJir73x/3UXb77fun3+z333pbPno/k5p7zOb/e38+5fd1zz7n3NFWFJKkvP3OwC5AkTZ7hLkkdMtwlqUOGuyR1yHCXpA6tONgFAKxatarWrl17sMuQpIeVq6+++ttVtXq+aYdEuK9du5Zdu3Yd7DIk6WElye0LTfO0jCR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SerQAcM9yQeT7E1y3Vjb0UkuT3Jze17Z2pPkfUl2J/lqkpOmWbwkaX6LOXL/MHD6nLazgR1VtR7Y0cYBXgysb48twPsnU6YkaSkOGO5V9Xngu3OaNwLb2vA24Myx9o/UyFXAUUmOmVSxkqTFWe4vVNdU1Z42fBewpg0fC3xrbL47Wtse5kiyhdHRPU960pOWWYaWau3Znxq0/DfOfcmEKlG33vnYAcveN7k6/p8bfEG1Rv+V05L/O6eq2lpVG6pqw+rV894aQZK0TMsN97v3n25pz3tb+53AE8fmO661SZJmaLnhfhmwqQ1vAi4da//t9q2ZU4D7xk7fSJJm5IDn3JNcBDwfWJXkDuAdwLnAJUk2A7cDL2+zfxo4A9gNfB94zRRqliQdwAHDvapescCk0+aZt4A3DC1KkjSMv1CVpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdWhQuCf5gyTXJ7kuyUVJjkyyLsnOJLuTXJzk8EkVK0lanGWHe5Jjgd8HNlTV04HDgLOA84Dzq+opwD3A5kkUKklavKGnZVYAP5tkBfBIYA9wKrC9Td8GnDlwG5KkJVp2uFfVncBfAt9kFOr3AVcD91bVA222O4Bj51s+yZYku5Ls2rdv33LLkCTNY8hpmZXARmAd8ATgUcDpi12+qrZW1Yaq2rB69erlliFJmseQ0zIvBG6rqn1V9SPgE8BzgKPaaRqA44A7B9YoSVqiIeH+TeCUJI9MEuA04AbgSuBlbZ5NwKXDSpQkLdWQc+47GV04/RJwbVvXVuBtwFuT7AYeB1w4gTolSUuw4sCzLKyq3gG8Y07zrcDJQ9YrSRrGX6hKUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOjQo3JMclWR7kq8luTHJs5IcneTyJDe355WTKlaStDhDj9zfC/xzVT0VeAZwI3A2sKOq1gM72rgkaYaWHe5JHgs8D7gQoKp+WFX3AhuBbW22bcCZQ4uUJC3NkCP3dcA+4ENJvpzkgiSPAtZU1Z42z13AmqFFSpKWZki4rwBOAt5fVScC/82cUzBVVUDNt3CSLUl2Jdm1b9++AWVIkuYaEu53AHdU1c42vp1R2N+d5BiA9rx3voWramtVbaiqDatXrx5QhiRprmWHe1XdBXwryS+2ptOAG4DLgE2tbRNw6aAKJUlLtmLg8m8CPprkcOBW4DWM3jAuSbIZuB14+cBtSJKWaFC4V9U1wIZ5Jp02ZL2SpGH8haokdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHVocLgnOSzJl5N8so2vS7Izye4kFyc5fHiZkqSlmMSR+5uBG8fGzwPOr6qnAPcAmyewDUnSEgwK9yTHAS8BLmjjAU4FtrdZtgFnDtmGJGnphh65vwf4I+AnbfxxwL1V9UAbvwM4dr4Fk2xJsivJrn379g0sQ5I0btnhnuTXgL1VdfVylq+qrVW1oao2rF69erllSJLmsWLAss8BXprkDOBI4DHAe4GjkqxoR+/HAXcOL1OStBTLPnKvqrdX1XFVtRY4C7iiql4JXAm8rM22Cbh0cJWSpCWZxvfc3wa8NcluRufgL5zCNiRJD2HIaZmfqqrPAZ9rw7cCJ09ivZKk5fEXqpLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOLTvckzwxyZVJbkhyfZI3t/ajk1ye5Ob2vHJy5UqSFmPIkfsDwB9W1QnAKcAbkpwAnA3sqKr1wI42LkmaoWWHe1XtqaovteH/Am4EjgU2AtvabNuAM4cWKUlamomcc0+yFjgR2Amsqao9bdJdwJoFltmSZFeSXfv27ZtEGZKkZnC4J3k08PfAW6rqe+PTqqqAmm+5qtpaVRuqasPq1auHliFJGjMo3JM8glGwf7SqPtGa705yTJt+DLB3WImSpKUa8m2ZABcCN1bVX41NugzY1IY3AZcuvzxJ0nKsGLDsc4DfAq5Nck1r+2PgXOCSJJuB24GXDytRkrRUyw73qvo3IAtMPm2565UkDecvVCWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUNTCfckpye5KcnuJGdPYxuSpIVNPNyTHAb8DfBi4ATgFUlOmPR2JEkLm8aR+8nA7qq6tap+CHwM2DiF7UiSFrBiCus8FvjW2PgdwK/MnSnJFmBLG70/yU3L3N4q4NvLXHaauqwr502wkgfrsr+m7FCtbfl1/WkmW8mD9ddf8OSFJkwj3BelqrYCW4euJ8muqtowgZImyrqWxrqW7lCtzbqWZlp1TeO0zJ3AE8fGj2ttkqQZmUa4/wewPsm6JIcDZwGXTWE7kqQFTPy0TFU9kOSNwL8AhwEfrKrrJ72dMYNP7UyJdS2NdS3doVqbdS3NVOpKVU1jvZKkg8hfqEpShwx3SerQwyLckxyd5PIkN7fnlQvM9+Mk17THZWPt65LsbLdDuLhd6J1JXUmemeTfk1yf5KtJfnNs2oeT3DZW8zMH1vOQt31IckT7+3e3/lg7Nu3trf2mJC8aUscy6nprkhta/+xI8uSxafPu0xnV9eok+8a2/7tj0za1/X5zkk0zruv8sZq+nuTesWnT7K8PJtmb5LoFpifJ+1rdX01y0ti0afbXgep6Zavn2iRfSPKMsWnfaO3XJNk147qen+S+sf31J2PTht/CpaoO+QfwbuDsNnw2cN4C892/QPslwFlt+APA62ZVF/ALwPo2/ARgD3BUG/8w8LIJ1XIYcAtwPHA48BXghDnzvB74QBs+C7i4DZ/Q5j8CWNfWc9gM63oB8Mg2/Lr9dT3UPp1RXa8G/nqeZY8Gbm3PK9vwylnVNWf+NzH60sJU+6ut+3nAScB1C0w/A/gMEOAUYOe0+2uRdT17//YY3RZl59i0bwCrDlJ/PR/45NDXwEKPh8WRO6PbF2xrw9uAMxe7YJIApwLbl7P80Lqq6utVdXMb/k9gL7B6Qtsft5jbPozXux04rfXPRuBjVfWDqroN2N3WN5O6qurKqvp+G72K0W8jpm3IbTJeBFxeVd+tqnuAy4HTD1JdrwAumtC2H1JVfR747kPMshH4SI1cBRyV5Bim218HrKuqvtC2C7N7fS2mvxYykVu4PFzCfU1V7WnDdwFrFpjvyCS7klyVZH/QPg64t6oeaON3MLpFwizrAiDJyYzeiW8Za/6z9pHx/CRHDKhlvts+zP07fzpP64/7GPXPYpadZl3jNjM6+ttvvn06y7p+o+2f7Un2/zjvkOivdvpqHXDFWPO0+msxFqp9mv21VHNfXwX8a5KrM7olyqw9K8lXknwmydNa20T666DdfmCuJJ8FHj/PpHPGR6qqkiz0/c0nV9WdSY4HrkhyLaMAO9h10Y5g/hbYVFU/ac1vZ/SmcDij77q+DXjXkHofzpK8CtgA/OpY8//Zp1V1y/xrmLh/Ai6qqh8k+T1Gn3pOndG2F+MsYHtV/Xis7WD21yEtyQsYhftzx5qf2/rr54HLk3ytHXHPwpcY7a/7k5wB/COwflIrP2SO3KvqhVX19HkelwJ3t3DcH5J7F1jHne35VuBzwInAdxh9PNz/Rrak2yFMoq4kjwE+BZzTPq7uX/ee9hH2B8CHGHYqZDG3ffjpPK0/Hsuof6Z5y4hFrTvJCxm9Yb609Qew4D6dSV1V9Z2xWi4Afnmxy06zrjFnMeeUzBT7azEWqv2g35IkyS8x2ocbq+o7+9vH+msv8A9M7nTkAVXV96rq/jb8aeARSVYxqf4acsFgVg/gL3jwhct3zzPPSuCINrwKuJl2EQL4OA++oPr6GdZ1OLADeMs8045pzwHeA5w7oJYVjC5UreN/L8I8bc48b+DBF1QvacNP48EXVG9lchdUF1PXiYxOVa1f7D6dUV3HjA3/OnBVGz4auK3Vt7INHz2rutp8T2V0MTCz6K+xbaxl4QuEL+HBF1S/OO3+WmRdT2J0HenZc9ofBfzc2PAXgNNnWNfj9+8/Rm8q32x9t6jXwAG3Pck/ZFoPRueFd7QX62f3vzAYfYS/oA0/G7i2dcS1wOax5Y8Hvth28Mf3/wOYUV2vAn4EXDP2eGabdkWr9Trg74BHD6znDODrjILynNb2LkZHwwBHtr9/d+uP48eWPactdxPw4gnvvwPV9Vng7rH+uexA+3RGdf05cH3b/pXAU8eW/Z3Wj7uB18yyrjb+TuYcDMygvy5i9G2vHzE6D7wZeC3w2jY9jP6jnlva9jfMqL8OVNcFwD1jr69drf341ldfafv5nBnX9cax19dVjL35zPcaWOrD2w9IUocOmXPukqTJMdwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtSh/4HhCqM02qvV28AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of labels\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot hist\n",
    "ax.hist(df.loc[df['company'] == 0, 'company'], bins = 10)\n",
    "ax.hist(df.loc[df['company'] == 1, 'company'], bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del gráfico anterior se tiene que las clases estan balanceadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "# Define own tokenizer\n",
    "\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def own_tokenizer(sentence):\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    doc = nlp(sentence.lower().strip())\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        # Filtering\n",
    "        if not token.is_space and not token.is_stop and not token.is_punct and not token.is_digit and not token.like_num:\n",
    "            \n",
    "            # add token to list in lemma form\n",
    "            tokens.append(token.lemma_)\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer\n",
    "\n",
    "# Bag of words\n",
    "bow_vector = CountVectorizer(tokenizer = own_tokenizer, ngram_range = (1,1))\n",
    "\n",
    "# TFIDF\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = own_tokenizer, ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "X = df['report'] # the features we want to analyze\n",
    "y = df['company'] # the labels, or answers, we want to test against\n",
    "\n",
    "# Split data in trainig and testing\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7)\n",
    "\n",
    "# Create split data\n",
    "sss = StratifiedShuffleSplit(n_splits= 1, test_size=0.3, random_state=0)\n",
    "\n",
    "# Generate index for splitting data\n",
    "# It get index first becuase it needs the index for word2vec implementation\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    \n",
    "    # Get index\n",
    "    train_id_list = train_index\n",
    "    test_id_list = test_index\n",
    "\n",
    "# Split data\n",
    "X_train, X_test = X[train_id_list], X[test_id_list]\n",
    "y_train, y_test = y[train_id_list], y[test_id_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log classifier\n",
    "log_regr = LogisticRegression()\n",
    "\n",
    "# Create pipeline\n",
    "pipe_bow = Pipeline([('preprocessing', bow_vector),\n",
    "                 ('logistic regression', log_regr)])\n",
    "\n",
    "pipe_tfidf = Pipeline([('preprocessing', tfidf_vector),\n",
    "                 ('logistic regression', log_regr)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/Desktop/master_UACH/env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/leo/Desktop/master_UACH/env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessing',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_p...,\n",
       "                                 tokenizer=<function own_tokenizer at 0x7f270ac66f28>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('regression-LR',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "# Using bag of words embedding\n",
    "pipe_bow.fit(X_train, y_train)\n",
    "\n",
    "# Using tfidf word embedding\n",
    "pipe_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on test data\n",
    "\n",
    "# using bag of words embedding\n",
    "y_predict_bow = pipe_bow.predict(X_test)\n",
    "\n",
    "# using tfidf embedding\n",
    "y_predict_tfid = pipe_tfidf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BOW embedding\n",
      "\n",
      "Logistic Regression Accuracy: 0.75\n",
      "Logistic Regression Precision: 0.6744186046511628\n",
      "Logistic Regression Recall: 0.9666666666666667\n",
      "Logistic Regression F1 score: 0.7945205479452055\n",
      "\n",
      "TFIDF embedding\n",
      "\n",
      "Logistic Regression Accuracy: 0.7333333333333333\n",
      "Logistic Regression Precision: 0.6944444444444444\n",
      "Logistic Regression Recall: 0.8333333333333334\n",
      "Logistic Regression F1 score: 0.7575757575757577\n"
     ]
    }
   ],
   "source": [
    "print('\\nBOW embedding\\n')\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, y_predict_bow))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, y_predict_bow))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, y_predict_bow))\n",
    "print(\"Logistic Regression F1 score:\",metrics.f1_score(y_test, y_predict_bow))\n",
    "\n",
    "print('\\nTFIDF embedding\\n')\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, y_predict_tfid))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, y_predict_tfid))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, y_predict_tfid))\n",
    "print(\"Logistic Regression F1 score:\",metrics.f1_score(y_test, y_predict_tfid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "# Define own tokenizer\n",
    "\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def own_tokenizer(sentence):\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    doc = nlp(sentence.lower().strip())\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        # Filtering\n",
    "        if not token.is_space and not token.is_stop and not token.is_punct and not token.is_digit and not token.like_num:\n",
    "            \n",
    "            # add token to list in lemma form\n",
    "            tokens.append(token.lemma_)\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v = [ own_tokenizer(report) for report in df['report']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v = np.array(X_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/Desktop/master_UACH/env/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# let X be a list of tokenized texts (i.e. list of lists of tokens)\n",
    "model_w2v = gensim.models.Word2Vec(X_w2v, size=11)\n",
    "w2v = dict(zip(model_w2v.wv.index2word, model_w2v.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "      \n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.values())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_word2vec = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"logistic regression\", LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X train and test\n",
    "\n",
    "# Get train data\n",
    "X_w2v_train = X_w2v[train_id_list]\n",
    "y_w2v_train = df['company'][train_id_list]\n",
    "\n",
    "# Get test data\n",
    "X_w2v_test = X_w2v[test_id_list]\n",
    "y_w2v_test = df['company'][test_id_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/Desktop/master_UACH/env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('word2vec vectorizer',\n",
       "                 <__main__.MeanEmbeddingVectorizer object at 0x7f26f7f9d5f8>),\n",
       "                ('logistic regression',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_word2vec.fit(X_w2v_train, y_w2v_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_w2v = pipe_word2vec.predict(X_w2v_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec embedding:\n",
      "\n",
      "Logistic Regression Accuracy: 0.6333333333333333\n",
      "Logistic Regression Precision: 0.6333333333333333\n",
      "Logistic Regression Recall: 0.6333333333333333\n",
      "Logistic Regression F1 score: 0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "print('\\nWord2Vec embedding:\\n')\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, y_predict_w2v))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, y_predict_w2v))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, y_predict_w2v))\n",
    "print(\"Logistic Regression F1 score:\",metrics.f1_score(y_test, y_predict_w2v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
