{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de embeddings para clasificación de noticias en medios chilenos (BOW, TDFIDF y W2V)\n",
    "\n",
    "1) Se entrena modelo Word2Vec para español.\n",
    "\n",
    "2) Se realiza embedding de noticas del medio \"La Cuarta\" y \"La Tercera\" utiliznado BOW, TFIDF, W2V\n",
    "\n",
    "3) Se entrenan algoritmos de clasificación de medio considernado el corpus de forma vectorizada por cada uno de los embeddings implementados anteriormente.\n",
    "\n",
    "4) Se analizan y comparan resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression # Log classifier\n",
    "from sklearn.model_selection import train_test_split # Split in train/test\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim \n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit # for split data (train/test)\n",
    "from stop_words import get_stop_words # stop words\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier # MLP NN\n",
    "from sklearn import svm # support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Esto te interesa... 22/05/2017 - Autor: La Cu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Esto te interesa... 01/06/2017 - Autor: Docto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siempre estuve esperando que llegara una muje...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Esto te interesa... 02/03/2017 - Autor: Claud...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Junto a Nachito Pop revivimos paso a paso la ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              report  company\n",
       "0   Esto te interesa... 22/05/2017 - Autor: La Cu...        0\n",
       "1   Esto te interesa... 01/06/2017 - Autor: Docto...        0\n",
       "2   Siempre estuve esperando que llegara una muje...        0\n",
       "3   Esto te interesa... 02/03/2017 - Autor: Claud...        0\n",
       "4   Junto a Nachito Pop revivimos paso a paso la ...        0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analize data\n",
    "# df = pd.read_csv('dataset-cuarta_vs_tercera.csv', sep = '|', header = None)\n",
    "df = pd.read_csv('MAD2019_dataset_lacuarta_latercera.csv', sep = '|', header = None)\n",
    "\n",
    "# Get news and medium\n",
    "# df = df.iloc[:,[1,3]]\n",
    "\n",
    "df = df.drop(labels = [0,2, 4], axis = 1)\n",
    "\n",
    "# Add names\n",
    "df.columns =  ['company', 'report']\n",
    "\n",
    "# company:\n",
    "# 0: La tercera\n",
    "# 1: La cuarta\n",
    "\n",
    "# Get dummies variables\n",
    "dummies = pd.get_dummies(df['company'], drop_first = True)\n",
    "df = pd.concat([df, dummies], axis=1) \n",
    "df.drop(['company'], inplace=True, axis=1)\n",
    "\n",
    "# rename again\n",
    "df.columns =  ['report', 'company']\n",
    "\n",
    "# display(dummies.head())\n",
    "# print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Taking smaller data\n",
    "df = df.sample(4000)\n",
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>report</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4447</td>\n",
       "      <td>Recuerdan los conventillos donde vivían los o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4113</td>\n",
       "      <td>Chile es el segundo país más rico de América ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2118</td>\n",
       "      <td>Investigadores chinos han desarrollado un mét...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3956</td>\n",
       "      <td>A lo largo de su campaña presidencial, Donald...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2798</td>\n",
       "      <td>Este domingo se desarrolló la ceremonia de lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                             report  company\n",
       "0   4447   Recuerdan los conventillos donde vivían los o...        1\n",
       "1   4113   Chile es el segundo país más rico de América ...        1\n",
       "2   2118   Investigadores chinos han desarrollado un mét...        1\n",
       "3   3956   A lo largo de su campaña presidencial, Donald...        1\n",
       "4   2798   Este domingo se desarrolló la ceremonia de lo...        1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying shape\n",
    "display(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analize classes (for check balancing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of labels\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot hist\n",
    "ax.hist(df.loc[df['company'] == 0, 'company'], bins = 10)\n",
    "ax.hist(df.loc[df['company'] == 1, 'company'], bins = 10)\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('N° instances')\n",
    "ax.title.set_text('N° instances of each class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tienen clases desbalanceadas, ya que la clase 'La tercera', posee mas instancias que la otra clase. Por lo que hay que tener precausión en su procesamiento, ya que por ejemplo, si se utilizara accuracy en clasificación, esta métrica no sería del todo correcta para comparar métodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tokenizer of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "# Define own tokenizer\n",
    "\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def own_tokenizer(sentence):\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    doc = nlp(sentence.lower().strip())\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        # Filtering\n",
    "        if not token.is_space and not token.is_stop and not token.is_punct and not token.is_digit and not token.like_num:\n",
    "            \n",
    "            # add token to list in lemma form\n",
    "            tokens.append(token.lemma_)\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW and TFIDF development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords to use in tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words (not has information)\n",
    "stop_words = ['a','e','i','o','u',' ','  ','   ','  ','$','  ','y', '-', 'q','c', 'nna', 'e', 'l', '=', '<', '>','x','href','/',\n",
    "             'JavaScript', 'navegador', '¿', 'Twitter', 'Tweets', 'web', 'URL']\n",
    "\n",
    "# using stop words from library\n",
    "stop_words_lib = get_stop_words('spanish')\n",
    "\n",
    "# joining stop words\n",
    "stop_words = stop_words + stop_words_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vectorizer of BOW and TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer\n",
    "\n",
    "# Bag of words\n",
    "bow_vector = CountVectorizer(tokenizer = own_tokenizer, ngram_range = (1,1))\n",
    "\n",
    "# TFIDF\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = own_tokenizer, ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data individally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "X = df['report'] # the features we want to analyze\n",
    "y = df['company'] # the labels, or answers, we want to test against"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crate pipelines for BOW and TFIDF:\n",
    "\n",
    "1) Apply the vectorization\n",
    "\n",
    "2) Model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipelines\n",
    "\n",
    "# Bag of words\n",
    "\n",
    "# Create pipeline using logistic regression\n",
    "pipe_bow_log = Pipeline([('bow vectorizer', bow_vector),\n",
    "                 ('logistic regression', LogisticRegression(solver = 'lbfgs'))])\n",
    "\n",
    "# Using extra trees classifier (it's base in decision tree (It's like a forest))\n",
    "pipe_bow_extra_trees = Pipeline([\n",
    "    (\"bow vectorizer\", bow_vector),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "# using MLP\n",
    "pipe_bow_MLP = Pipeline([\n",
    "    (\"bow vectorizer\", bow_vector),\n",
    "    (\"MLP\", MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(1000, 2), random_state=1))\n",
    "])\n",
    "\n",
    "# Using SVM\n",
    "pipe_bow_SVM = Pipeline([\n",
    "    (\"bow vectorizer\", bow_vector),\n",
    "    (\"SVM\", svm.SVC(gamma='scale'))\n",
    "])\n",
    "\n",
    "# TFIDF\n",
    "\n",
    "# log reg using tfidf\n",
    "pipe_tfidf_log = Pipeline([('tfidf vectorizer', tfidf_vector),\n",
    "                 ('logistic regression', LogisticRegression(solver = 'lbfgs'))])\n",
    "\n",
    "# Using extra trees classifier (it's base in decision tree (It's like a forest))\n",
    "pipe_tfidf_extra_trees = Pipeline([\n",
    "    (\"tfidf vectorizer\", tfidf_vector),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "# using MLP\n",
    "pipe_tfidf_MLP = Pipeline([\n",
    "    (\"tfidf vectorizer\", tfidf_vector),\n",
    "    (\"MLP\", MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(1000, 2), random_state=1))\n",
    "])\n",
    "\n",
    "# Using SVM\n",
    "pipe_tfidf_SVM = Pipeline([\n",
    "    (\"tfidf vectorizer\", tfidf_vector),\n",
    "    (\"SVM\", svm.SVC(gamma='scale'))\n",
    "])\n",
    "\n",
    "\n",
    "# LIST OF MODELS\n",
    "\n",
    "# list of models\n",
    "models_bow_tfidf = [\n",
    "    ('bow + log reg', pipe_bow_log),\n",
    "    ('bow + ext_tree', pipe_bow_extra_trees),\n",
    "    ('bow + MLP', pipe_bow_MLP),\n",
    "    ('bow + SVM', pipe_bow_SVM),\n",
    "    \n",
    "    ('tfidf + log reg', pipe_tfidf_log),\n",
    "    ('tfidf + ext_tree', pipe_tfidf_extra_trees),\n",
    "    ('tfidf + MLP', pipe_tfidf_MLP),\n",
    "    ('tfidf + SVM', pipe_tfidf_SVM)\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train each model (bow and tfidf) considering differentes sizes of trainig data. In each training, it gets the accuracy of each model and store in scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model function (this was done for parallelizing models)\n",
    "def train_model(item):\n",
    "    \n",
    "    name = item[0]\n",
    "    model = item[1]\n",
    "    \n",
    "    print('Train model {0} with PID: {1}'.format(name, os.getpid()))\n",
    "    \n",
    "    # restart scores for each model\n",
    "    score = []\n",
    "    \n",
    "    # differents size of training examples\n",
    "    num_train = np.linspace(start = 2, stop = y.size - 10, num = 5).astype(int)\n",
    "    \n",
    "    # iterate over each train size\n",
    "    for train_size in num_train:\n",
    "\n",
    "        print('train size of {0}: {1}\\n'.format(name, train_size))\n",
    "\n",
    "        test_size = 1 - (train_size / float(len(y)))\n",
    "\n",
    "        sp = StratifiedShuffleSplit(n_splits=1, test_size = test_size)\n",
    "\n",
    "        sp.get_n_splits(X, y)\n",
    "\n",
    "        for train, test in sp.split(X, y):\n",
    "\n",
    "            X_train, X_test = X[train], X[test]\n",
    "            y_train, y_test = y[train], y[test]\n",
    "\n",
    "            acc = (metrics.accuracy_score(model.fit(X_train, y_train).predict(X_test), y_test))\n",
    "\n",
    "            score.append(acc)\n",
    "    \n",
    "        \n",
    "    # add scores to list\n",
    "    results = {\n",
    "        'name': name,\n",
    "        'accuracy': score\n",
    "    }\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model bow + MLP with PID: 5177\n",
      "Train model tfidf + log reg with PID: 5179\n",
      "Train model tfidf + ext_tree with PID: 5180\n",
      "Train model bow + ext_tree with PID: 5176\n",
      "Train model bow + log reg with PID: 5175\n",
      "Train model bow + SVM with PID: 5178\n",
      "Train model tfidf + MLP with PID: 5181\n",
      "Train model tfidf + SVM with PID: 5182\n",
      "train size of bow + MLP: 2\n",
      "train size of bow + SVM: 2\n",
      "train size of tfidf + log reg: 2\n",
      "train size of bow + ext_tree: 2\n",
      "\n",
      "\n",
      "\n",
      "train size of tfidf + MLP: 2\n",
      "train size of tfidf + SVM: 2\n",
      "train size of tfidf + ext_tree: 2\n",
      "\n",
      "\n",
      "\n",
      "train size of bow + log reg: 2\n",
      "\n",
      "\n",
      "train size of tfidf + MLP: 999\n",
      "\n",
      "train size of bow + ext_tree: 999\n",
      "\n",
      "train size of tfidf + SVM: 999\n",
      "\n",
      "train size of tfidf + log reg: 999\n",
      "\n",
      "train size of bow + MLP: 999\n",
      "\n",
      "train size of tfidf + ext_tree: 999\n",
      "\n",
      "train size of bow + log reg: 999\n",
      "\n",
      "train size of bow + SVM: 999\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/Desktop/master_UACH/env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size of tfidf + log reg: 1996\n",
      "\n",
      "train size of tfidf + ext_tree: 1996\n",
      "\n",
      "train size of bow + ext_tree: 1996\n",
      "\n",
      "train size of bow + log reg: 1996\n",
      "\n",
      "train size of tfidf + SVM: 1996\n",
      "\n",
      "train size of bow + SVM: 1996\n",
      "\n",
      "train size of tfidf + MLP: 1996\n",
      "\n",
      "train size of bow + MLP: 1996\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/Desktop/master_UACH/env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size of bow + ext_tree: 2993\n",
      "\n",
      "train size of tfidf + ext_tree: 2993\n",
      "\n",
      "train size of tfidf + log reg: 2993\n",
      "\n",
      "train size of bow + log reg: 2993\n",
      "\n",
      "train size of bow + SVM: 2993\n",
      "\n",
      "train size of tfidf + SVM: 2993\n",
      "\n",
      "train size of tfidf + MLP: 2993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/Desktop/master_UACH/env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size of bow + ext_tree: 3990\n",
      "\n",
      "train size of tfidf + ext_tree: 3990\n",
      "\n",
      "train size of tfidf + log reg: 3990\n",
      "\n",
      "train size of bow + log reg: 3990\n",
      "\n",
      "train size of bow + SVM: 3990\n",
      "\n",
      "train size of tfidf + SVM: 3990\n",
      "\n",
      "train size of tfidf + MLP: 3990\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/Desktop/master_UACH/env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "\n",
    "# using parallelism\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "# Init pool of process\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "# Exec function in each process\n",
    "scores_bow_tfidf = pool.map(train_model, models_bow_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data in format to w2v (tokenize each doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each doc\n",
    "X_w2v = [ own_tokenizer(report) for report in df['report']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform list to ndarray\n",
    "X_w2v = np.array(X_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get w2v model and the representation of each word in w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of w2v\n",
    "w2v_dim = 500\n",
    "\n",
    "# Create w2v model\n",
    "model_w2v = gensim.models.Word2Vec(X_w2v, size = w2v_dim)\n",
    "\n",
    "# Dictionary: key: each word, value: its w2v representation (vector)\n",
    "w2v = dict(zip(model_w2v.wv.index2word, model_w2v.wv.syn0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding of each doc (to format w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding of docs using w2v\n",
    "# this get a vector of each document of dimension (dimension_vector_w2v, 1), so the total array is (n_docs, dimension_vector_w2v)\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "      \n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.values())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        # This mean each dimension of each word in doc, it getting the w2v representation of each doc\n",
    "        doc_w2v = np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "        \n",
    "#         print(doc_w2v.shape)\n",
    "#         print(doc_w2v)\n",
    "        # Return vlaue\n",
    "        return doc_w2v\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines for w2v:\n",
    "\n",
    "1) Apply w2v transformation\n",
    "\n",
    "2) Apply model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe for traing using w2v\n",
    "\n",
    "# using logistic regression\n",
    "pipe_word2vec_log = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"logistic regression\", LogisticRegression(solver = 'lbfgs'))])\n",
    "\n",
    "# Using extra trees classifier (it's base in decision tree (It's like a forest))\n",
    "pipe_word2vec_extra_trees = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "# using MLP\n",
    "pipe_word2vec_MLP = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"MLP\", MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(1000, 2), random_state=1))\n",
    "])\n",
    "\n",
    "# Using SVM\n",
    "pipe_word2vec_SVM = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"SVM\", svm.SVC(gamma='scale'))\n",
    "])\n",
    "\n",
    "# list of models\n",
    "# Add new model here\n",
    "models_w2v = [\n",
    "    ('w2v + log reg', pipe_word2vec_log), \n",
    "    ('w2v + ext_trees', pipe_word2vec_extra_trees), \n",
    "    ('w2v + MLP', pipe_word2vec_MLP),\n",
    "    ('w2v + SVM', pipe_word2vec_SVM),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train each model using w2v representation. Training is by differentes training sizes. For each model, it gets the accuracy and add it to score list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model function (this was done for parallelizing models)\n",
    "def train_model_w2v(item):\n",
    "    \n",
    "    name = item[0]\n",
    "    model = item[1]\n",
    "    \n",
    "    print('Train model {0} with PID: {1}'.format(name, os.getpid()))\n",
    "    \n",
    "    # restart scores for each model\n",
    "    score = []\n",
    "    \n",
    "    # differents size of training examples\n",
    "    num_train = np.linspace(start = 2, stop = y.size - 10, num = 5).astype(int)\n",
    "    \n",
    "    # iterate over each train size\n",
    "    for train_size in num_train:\n",
    "\n",
    "        print('train size of {0}: {1}\\n'.format(name, train_size))\n",
    "\n",
    "        test_size = 1 - (train_size / float(len(y_w2v)))\n",
    "\n",
    "        sp = StratifiedShuffleSplit(n_splits=1, test_size = test_size)\n",
    "\n",
    "        sp.get_n_splits(X_w2v, y_w2v)\n",
    "\n",
    "        for train, test in sp.split(X_w2v, y_w2v):\n",
    "\n",
    "            X_train_w2v, X_test_w2v = X[train], X[test]\n",
    "            y_train_w2v, y_test_w2v = y[train], y[test]\n",
    "            \n",
    "            acc = (metrics.accuracy_score(model.fit(X_train_w2v, y_train_w2v).predict(X_test_w2v), y_test_w2v))\n",
    "\n",
    "            score.append(acc)\n",
    "    \n",
    "        \n",
    "    # add scores to list\n",
    "    results = {\n",
    "        'name': name,\n",
    "        'accuracy': score\n",
    "    }\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "\n",
    "# using parallelism\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "# get labels\n",
    "y_w2v = df['company']\n",
    "\n",
    "# Init pool of process\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "# Exec function in each process\n",
    "scores_w2v = pool.map(train_model_w2v, models_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join scores\n",
    "scores = scores_bow_tfidf + scores_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare results from bow, tfidf and w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x axis values\n",
    "num_train = np.linspace(start = 2, stop = y.size - 10, num = 5).astype(int)\n",
    "\n",
    "# Crate figure\n",
    "fig, ax = plt.subplots(1,4, figsize = (17,5))\n",
    "\n",
    "# Log regression\n",
    "num = 0\n",
    "ax[num].plot(num_train, next(item for item in scores if item[\"name\"] == \"bow + log reg\")['accuracy'], label = next(item for item in scores if item[\"name\"] == \"bow + log reg\")['name'], marker = '.')\n",
    "ax[num].plot(num_train, next(item for item in scores if item[\"name\"] == \"tfidf + log reg\")['accuracy'], label = next(item for item in scores if item[\"name\"] == \"tfidf + log reg\")['name'], marker = '.')\n",
    "ax[num].plot(num_train, next(item for item in scores if item[\"name\"] == \"w2v + log reg\")['accuracy'], linewidth = 5, label = next(item for item in scores if item[\"name\"] == \"w2v + log reg\")['name'], marker = '.')\n",
    "ax[num].legend()\n",
    "ax[num].title.set_text('Logistic Regression')\n",
    "ax[num].set_xlabel('Training data')\n",
    "ax[num].set_ylabel('Accuracy')\n",
    "\n",
    "# extra trees\n",
    "num = 1\n",
    "ax[num].plot(num_train, next(item for item in scores if item[\"name\"] == \"bow + ext_tree\")['accuracy'], label = next(item for item in scores if item[\"name\"] == \"bow + ext_tree\")['name'], marker = '.')\n",
    "ax[num].plot(num_train, next(item for item in scores if item[\"name\"] == \"tfidf + ext_tree\")['accuracy'], label = next(item for item in scores if item[\"name\"] == \"tfidf + ext_tree\")['name'], marker = '.')\n",
    "ax[num].plot(num_train, next(item for item in scores if item[\"name\"] == \"w2v + ext_trees\")['accuracy'], linewidth = 5,label = next(item for item in scores if item[\"name\"] == \"w2v + ext_trees\")['name'], marker = '.')\n",
    "ax[num].legend()\n",
    "ax[num].title.set_text('Extra trees')\n",
    "ax[num].set_xlabel('Training data')\n",
    "ax[num].set_ylabel('Accuracy')\n",
    "\n",
    "# MLP\n",
    "num = 2\n",
    "ax[num].plot(num_train, next(item for item in scores if item[\"name\"] == \"bow + MLP\")['accuracy'], label = next(item for item in scores if item[\"name\"] == \"bow + MLP\")['name'], marker = '.')\n",
    "ax[num].plot(num_train, next(item for item in scores if item[\"name\"] == \"tfidf + MLP\")['accuracy'], label = next(item for item in scores if item[\"name\"] == \"tfidf + MLP\")['name'], marker = '.')\n",
    "ax[num].plot(num_train, next(item for item in scores if item[\"name\"] == \"w2v + MLP\")['accuracy'], linewidth = 5,label = next(item for item in scores if item[\"name\"] == \"w2v + MLP\")['name'], marker = '.')\n",
    "ax[num].legend()\n",
    "ax[num].title.set_text('MLP')\n",
    "ax[num].set_xlabel('Training data')\n",
    "ax[num].set_ylabel('Accuracy')\n",
    "\n",
    "# MLP\n",
    "num = 3\n",
    "ax[num].plot(num_train, next(item for item in scores if item[\"name\"] == \"bow + SVM\")['accuracy'], label = next(item for item in scores if item[\"name\"] == \"bow + SVM\")['name'], marker = '.')\n",
    "ax[num].plot(num_train, next(item for item in scores if item[\"name\"] == \"tfidf + SVM\")['accuracy'], label = next(item for item in scores if item[\"name\"] == \"tfidf + SVM\")['name'], marker = '.')\n",
    "ax[num].plot(num_train, next(item for item in scores if item[\"name\"] == \"w2v + SVM\")['accuracy'], linewidth = 5,label = next(item for item in scores if item[\"name\"] == \"w2v + SVM\")['name'], marker = '.')\n",
    "ax[num].legend()\n",
    "ax[num].title.set_text('SVM')\n",
    "ax[num].set_xlabel('Training data')\n",
    "ax[num].set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discusion\n",
    "\n",
    "#### Regresión logística:\n",
    "\n",
    "Para el caso de embedding utilizando BOW, se requiere solamente 50 ejemplos para alcanzar accuracy altos (superiores a 0,9) y que agregar mas datos para este problema de clasificación logra aumentar el acurracy pero en poca cantidad. Al tener el máximo número de datos de entrenamiento (Se escogen 10 menos que el maximo conjunto de datos), el modelo tiende a sobreajustarse al training set por lo que disminuye el accuracy sobre el testing set. \n",
    "\n",
    "Utilizando TFIDF se obtiene accuracies menores que BOW, lo que sugiere que en esta tarea de clasificación, hacer un embedding que de importancia a las palabras mas relevantes (palabras que logran diferenciar entre documentos) no aporta para la clasificación con logistic regression.\n",
    "\n",
    "Finalmente, para el caso de w2v, se tienen accuracies cercanas a 0,7, las cuales son las menores comparadas con BOW y TFIDF (para 50 o mas datos de entrenamiento). Esto sugiere que el embedding usando w2v no aporta para esta tarea de clasificación, lo que significa que el algoritmo no considera relevante el significado de las palabras (propósito de w2v) para clasificar por medios. \n",
    "\n",
    "Por otro lado, cabe destacar que w2v mantuvo un accuracy cercano a 0,7 para todo conjunto de datos (incluyendo 2 datos de entrenamiento), sin embargo los otros embeddings poseen menores accuracies para pocos datos de entrenamiento.\n",
    "\n",
    "#### Extra trees\n",
    "\n",
    "Utilizando BOW y TFIDF se obtiene accuracies similares para todo tamaño de conjunto de entrenamiento. En cambio, utilizando w2v, se tienen nuevamente acurracies cercanas a 0,7, lo que sugiere que el algoritmo no considera relevante el significado de las palabras para clasificar.\n",
    "\n",
    "#### MLP\n",
    "\n",
    "Nuevamente BOW y TFIDF poseen accuracies similares, sin embargo BOW obtiene en mayores accuracies para algunos largos de train data específicos (50). Para el caso de w2v se tienen accuracies mas irregulares que en los otros algoritmos, ya que varía entre 0,35 y 0,68. Sin embargo para todo largo de train size se tiene que W2V posee menor accuracy que los otros embeddings.\n",
    "\n",
    "#### SVM\n",
    "\n",
    "En este caso, TFIDF posee mayor accuracy que los otros embeddings (excepto para 50). Para el caso de BOW, se tiene menores accuracies que TFIDF. Y para el caso de W2V, para todo largo se tienen menores accuracies, excepto para largo pequeño, en donde los 3 embeddings poseens acurracies similares.\n",
    "\n",
    "#### Conclusiones generales\n",
    "\n",
    "De los resultados y del análisis obtenido, se tiene que para la tarea de clasificación actual, utilizar embeddings BOW o TFIDF permite obtener mayores accuracies que utilizando W2V. Esto implica que en los algoritmos utilizados, estos no consideran relevante la información del significado de las palabras, ya que se obtienen menores accuracies utilizando W2V y, por otro lado, solo realizando un conteo de las palabras dentro de los documentos (BOW) se pueden obtener altos valores de accuracies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
